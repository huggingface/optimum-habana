1a2
> # coding=utf-8
16c17,19
< """Conditional text generation with the auto-regressive models of the library (GPT/GPT-2/CTRL/Transformer-XL/XLNet)"""
---
> """
> Conditional text generation on Habana Gaudi/Gaudi2.
> """
19c22
< import inspect
---
> import json
20a24,28
> import math
> import os
> import time
> from itertools import cycle
> from pathlib import Path
23,46c31,38
< from accelerate import PartialState
< from accelerate.utils import set_seed
< 
< from transformers import (
<     AutoTokenizer,
<     BloomForCausalLM,
<     BloomTokenizerFast,
<     CTRLLMHeadModel,
<     CTRLTokenizer,
<     GenerationMixin,
<     GPT2LMHeadModel,
<     GPT2Tokenizer,
<     GPTJForCausalLM,
<     LlamaForCausalLM,
<     LlamaTokenizer,
<     OpenAIGPTLMHeadModel,
<     OpenAIGPTTokenizer,
<     OPTForCausalLM,
<     TransfoXLLMHeadModel,
<     TransfoXLTokenizer,
<     XLMTokenizer,
<     XLMWithLMHeadModel,
<     XLNetLMHeadModel,
<     XLNetTokenizer,
---
> from transformers import BatchEncoding
> from utils import (
>     SetTrueOrFalseOrNone,
>     adjust_batch,
>     count_hpu_graphs,
>     finalize_quantization,
>     initialize_model,
>     save_model,
48c40,41
< from transformers.modeling_outputs import CausalLMOutputWithPast
---
> 
> from optimum.habana.utils import get_hpu_memory_stats
58,280d50
< MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop
< 
< MODEL_CLASSES = {
<     "gpt2": (GPT2LMHeadModel, GPT2Tokenizer),
<     "ctrl": (CTRLLMHeadModel, CTRLTokenizer),
<     "openai-gpt": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),
<     "xlnet": (XLNetLMHeadModel, XLNetTokenizer),
<     "transfo-xl": (TransfoXLLMHeadModel, TransfoXLTokenizer),
<     "xlm": (XLMWithLMHeadModel, XLMTokenizer),
<     "gptj": (GPTJForCausalLM, AutoTokenizer),
<     "bloom": (BloomForCausalLM, BloomTokenizerFast),
<     "llama": (LlamaForCausalLM, LlamaTokenizer),
<     "opt": (OPTForCausalLM, GPT2Tokenizer),
< }
< 
< # Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia
< # in https://github.com/rusiaaman/XLNet-gen#methodology
< # and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e
< PREFIX = """In 1991, the remains of Russian Tsar Nicholas II and his family
< (except for Alexei and Maria) are discovered.
< The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the
< remainder of the story. 1883 Western Siberia,
< a young Grigori Rasputin is asked by his father and a group of men to perform magic.
< Rasputin has a vision and denounces one of the men as a horse thief. Although his
< father initially slaps him for making such an accusation, Rasputin watches as the
< man is chased outside and beaten. Twenty years later, Rasputin sees a vision of
< the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,
< with people, even a bishop, begging for his blessing. <eod> </s> <eos>"""
< 
< 
< #
< # Functions to prepare models' input
< #
< 
< 
< def prepare_ctrl_input(args, _, tokenizer, prompt_text):
<     if args.temperature > 0.7:
<         logger.info("CTRL typically works better with lower temperatures (and lower top_k).")
< 
<     encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)
<     if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):
<         logger.info("WARNING! You are not starting your generation from a control code so you won't get good results")
<     return prompt_text
< 
< 
< def prepare_xlm_input(args, model, tokenizer, prompt_text):
<     # kwargs = {"language": None, "mask_token_id": None}
< 
<     # Set the language
<     use_lang_emb = hasattr(model.config, "use_lang_emb") and model.config.use_lang_emb
<     if hasattr(model.config, "lang2id") and use_lang_emb:
<         available_languages = model.config.lang2id.keys()
<         if args.xlm_language in available_languages:
<             language = args.xlm_language
<         else:
<             language = None
<             while language not in available_languages:
<                 language = input("Using XLM. Select language in " + str(list(available_languages)) + " >>> ")
< 
<         model.config.lang_id = model.config.lang2id[language]
<         # kwargs["language"] = tokenizer.lang2id[language]
< 
<     # TODO fix mask_token_id setup when configurations will be synchronized between models and tokenizers
<     # XLM masked-language modeling (MLM) models need masked token
<     # is_xlm_mlm = "mlm" in args.model_name_or_path
<     # if is_xlm_mlm:
<     #     kwargs["mask_token_id"] = tokenizer.mask_token_id
< 
<     return prompt_text
< 
< 
< def prepare_xlnet_input(args, _, tokenizer, prompt_text):
<     prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX
<     prompt_text = prefix + prompt_text
<     return prompt_text
< 
< 
< def prepare_transfoxl_input(args, _, tokenizer, prompt_text):
<     prefix = args.prefix if args.prefix else args.padding_text if args.padding_text else PREFIX
<     prompt_text = prefix + prompt_text
<     return prompt_text
< 
< 
< PREPROCESSING_FUNCTIONS = {
<     "ctrl": prepare_ctrl_input,
<     "xlm": prepare_xlm_input,
<     "xlnet": prepare_xlnet_input,
<     "transfo-xl": prepare_transfoxl_input,
< }
< 
< 
< def adjust_length_to_model(length, max_sequence_length):
<     if length < 0 and max_sequence_length > 0:
<         length = max_sequence_length
<     elif 0 < max_sequence_length < length:
<         length = max_sequence_length  # No generation bigger than model size
<     elif length < 0:
<         length = MAX_LENGTH  # avoid infinite loop
<     return length
< 
< 
< def sparse_model_config(model_config):
<     embedding_size = None
<     if hasattr(model_config, "hidden_size"):
<         embedding_size = model_config.hidden_size
<     elif hasattr(model_config, "n_embed"):
<         embedding_size = model_config.n_embed
<     elif hasattr(model_config, "n_embd"):
<         embedding_size = model_config.n_embd
< 
<     num_head = None
<     if hasattr(model_config, "num_attention_heads"):
<         num_head = model_config.num_attention_heads
<     elif hasattr(model_config, "n_head"):
<         num_head = model_config.n_head
< 
<     if embedding_size is None or num_head is None or num_head == 0:
<         raise ValueError("Check the model config")
< 
<     num_embedding_size_per_head = int(embedding_size / num_head)
<     if hasattr(model_config, "n_layer"):
<         num_layer = model_config.n_layer
<     elif hasattr(model_config, "num_hidden_layers"):
<         num_layer = model_config.num_hidden_layers
<     else:
<         raise ValueError("Number of hidden layers couldn't be determined from the model config")
< 
<     return num_layer, num_head, num_embedding_size_per_head
< 
< 
< def generate_past_key_values(model, batch_size, seq_len):
<     num_block_layers, num_attention_heads, num_embedding_size_per_head = sparse_model_config(model.config)
<     if model.config.model_type == "bloom":
<         past_key_values = tuple(
<             (
<                 torch.empty(int(num_attention_heads * batch_size), num_embedding_size_per_head, seq_len)
<                 .to(model.dtype)
<                 .to(model.device),
<                 torch.empty(int(num_attention_heads * batch_size), seq_len, num_embedding_size_per_head)
<                 .to(model.dtype)
<                 .to(model.device),
<             )
<             for _ in range(num_block_layers)
<         )
<     else:
<         past_key_values = tuple(
<             (
<                 torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)
<                 .to(model.dtype)
<                 .to(model.device),
<                 torch.empty(batch_size, num_attention_heads, seq_len, num_embedding_size_per_head)
<                 .to(model.dtype)
<                 .to(model.device),
<             )
<             for _ in range(num_block_layers)
<         )
<     return past_key_values
< 
< 
< def prepare_jit_inputs(inputs, model, tokenizer):
<     batch_size = len(inputs)
<     dummy_input = tokenizer.batch_encode_plus(inputs, return_tensors="pt")
<     dummy_input = dummy_input.to(model.device)
<     if model.config.use_cache:
<         dummy_input["past_key_values"] = generate_past_key_values(model, batch_size, 1)
<     dummy_input["attention_mask"] = torch.cat(
<         [
<             torch.zeros(dummy_input["attention_mask"].shape[0], 1)
<             .to(dummy_input["attention_mask"].dtype)
<             .to(model.device),
<             dummy_input["attention_mask"],
<         ],
<         -1,
<     )
<     return dummy_input
< 
< 
< class _ModelFallbackWrapper(GenerationMixin):
<     __slots__ = ("_optimized", "_default")
< 
<     def __init__(self, optimized, default):
<         self._optimized = optimized
<         self._default = default
< 
<     def __call__(self, *args, **kwargs):
<         if kwargs["past_key_values"] is None and self._default.config.use_cache:
<             kwargs["past_key_values"] = generate_past_key_values(self._default, kwargs["input_ids"].shape[0], 0)
<         kwargs.pop("position_ids", None)
<         for k in list(kwargs.keys()):
<             if kwargs[k] is None or isinstance(kwargs[k], bool):
<                 kwargs.pop(k)
<         outputs = self._optimized(**kwargs)
<         lm_logits = outputs[0]
<         past_key_values = outputs[1]
<         fixed_output = CausalLMOutputWithPast(
<             loss=None,
<             logits=lm_logits,
<             past_key_values=past_key_values,
<             hidden_states=None,
<             attentions=None,
<         )
<         return fixed_output
< 
<     def __getattr__(self, item):
<         return getattr(self._default, item)
< 
<     def prepare_inputs_for_generation(
<         self, input_ids, past_key_values=None, inputs_embeds=None, use_cache=None, **kwargs
<     ):
<         return self._default.prepare_inputs_for_generation(
<             input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, **kwargs
<         )
< 
<     def _reorder_cache(
<         self, past_key_values: tuple[tuple[torch.Tensor]], beam_idx: torch.Tensor
<     ) -> tuple[tuple[torch.Tensor]]:
<         """
<         This function is used to re-order the `past_key_values` cache if [`~PretrainedModel.beam_search`] or
<         [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct
<         beam_idx at every generation step.
<         """
<         return self._default._reorder_cache(past_key_values, beam_idx)
< 
282,283c52,54
< def main():
<     parser = argparse.ArgumentParser()
---
> def setup_parser(parser):
>     # Arguments management
>     parser.add_argument("--device", "-d", type=str, choices=["hpu"], help="Device to run", default="hpu")
285c56
<         "--model_type",
---
>         "--model_name_or_path",
289c60
<         help="Model type selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
---
>         help="Path to pre-trained model (on the HF Hub or locally).",
292c63,91
<         "--model_name_or_path",
---
>         "--bf16",
>         action="store_true",
>         help="Whether to perform generation in bf16 precision.",
>     )
>     parser.add_argument("--max_new_tokens", type=int, default=100, help="Number of tokens to generate.")
>     parser.add_argument(
>         "--max_input_tokens",
>         type=int,
>         default=0,
>         help="If > 0 then pad and truncate the input sequences to this specified length of tokens. \
>             if == 0, then truncate to 16 (original default) \
>             if < 0, then do not truncate, use full input prompt",
>     )
>     parser.add_argument("--batch_size", type=int, default=1, help="Input batch size.")
>     parser.add_argument("--warmup", type=int, default=3, help="Number of warmup iterations for benchmarking.")
>     parser.add_argument("--n_iterations", type=int, default=5, help="Number of inference iterations for benchmarking.")
>     parser.add_argument("--local_rank", type=int, default=0, metavar="N", help="Local process rank.")
>     parser.add_argument(
>         "--use_kv_cache",
>         action="store_true",
>         help="Whether to use the key/value cache for decoding. It should speed up generation.",
>     )
>     parser.add_argument(
>         "--use_hpu_graphs",
>         action="store_true",
>         help="Whether to use HPU graphs or not. Using HPU graphs should give better latencies.",
>     )
>     parser.add_argument(
>         "--dataset_name",
295,296c94,117
<         required=True,
<         help="Path to pre-trained model or shortcut name selected in the list: " + ", ".join(MODEL_CLASSES.keys()),
---
>         help="Optional argument if you want to assess your model on a given dataset of the HF Hub.",
>     )
>     parser.add_argument(
>         "--column_name",
>         default=None,
>         type=str,
>         help="If `--dataset_name` was given, this will be the name of the column to use as prompts for generation.",
>     )
>     parser.add_argument(
>         "--do_sample",
>         action="store_true",
>         help="Whether to use sampling for generation.",
>     )
>     parser.add_argument(
>         "--num_beams",
>         default=1,
>         type=int,
>         help="Number of beams used for beam search generation. 1 means greedy search will be performed.",
>     )
>     parser.add_argument(
>         "--top_k",
>         default=None,
>         type=int,
>         help="Size of candidate set used for re-ranking in contrastive search. top_k > 1 enables contrastive search.",
298,302d118
< 
<     parser.add_argument("--prompt", type=str, default="")
<     parser.add_argument("--length", type=int, default=20)
<     parser.add_argument("--stop_token", type=str, default=None, help="Token at which text generation is stopped")
< 
304c120,121
<         "--temperature",
---
>         "--penalty_alpha",
>         default=None,
306,307c123
<         default=1.0,
<         help="temperature of 1.0 has no effect, lower tend toward greedy sampling",
---
>         help="Degeneration penalty for contrastive search. penalty_alpha > 0 enables contrastive search.",
310c126,146
<         "--repetition_penalty", type=float, default=1.0, help="primarily useful for CTRL model; in that case, use 1.2"
---
>         "--trim_logits",
>         action="store_true",
>         help="Calculate logits only for the last token to save memory in the first step.",
>     )
>     parser.add_argument(
>         "--seed",
>         default=27,
>         type=int,
>         help="Seed to use for random generation. Useful to reproduce your runs with `--do_sample`.",
>     )
>     parser.add_argument(
>         "--profiling_warmup_steps",
>         default=0,
>         type=int,
>         help="Number of steps to ignore for profiling.",
>     )
>     parser.add_argument(
>         "--profiling_steps",
>         default=0,
>         type=int,
>         help="Number of steps to capture for profiling.",
312,319d147
<     parser.add_argument("--k", type=int, default=0)
<     parser.add_argument("--p", type=float, default=0.9)
< 
<     parser.add_argument("--prefix", type=str, default="", help="Text added prior to input.")
<     parser.add_argument("--padding_text", type=str, default="", help="Deprecated, the use of `--prefix` is preferred.")
<     parser.add_argument("--xlm_language", type=str, default="", help="Optional language when used with the XLM model.")
< 
<     parser.add_argument("--seed", type=int, default=42, help="random seed for initialization")
321c149
<         "--use_cpu",
---
>         "--profiling_record_shapes",
323c151
<         help="Whether or not to use cpu. If set to False, we will use gpu/npu or mps device if available",
---
>         help="Record shapes when enabling profiling.",
325d152
<     parser.add_argument("--num_return_sequences", type=int, default=1, help="The number of samples to generate.")
327c154,201
<         "--fp16",
---
>         "--prompt",
>         default=None,
>         type=str,
>         nargs="*",
>         help='Optional argument to give a prompt of your choice as input. Can be a single string (eg: --prompt "Hello world"), or a list of space-separated strings (eg: --prompt "Hello world" "How are you?")',
>     )
>     parser.add_argument(
>         "--bad_words",
>         default=None,
>         type=str,
>         nargs="+",
>         help="Optional argument list of words that are not allowed to be generated.",
>     )
>     parser.add_argument(
>         "--force_words",
>         default=None,
>         type=str,
>         nargs="+",
>         help="Optional argument list of words that must be generated.",
>     )
>     parser.add_argument(
>         "--assistant_model",
>         default=None,
>         type=str,
>         help="Optional argument to give a path to a draft/assistant model for assisted decoding.",
>     )
>     parser.add_argument(
>         "--peft_model",
>         default=None,
>         type=str,
>         help="Optional argument to give a path to a PEFT model.",
>     )
>     parser.add_argument("--num_return_sequences", type=int, default=1)
>     parser.add_argument(
>         "--token",
>         default=None,
>         type=str,
>         help="The token to use as HTTP bearer authorization for remote files. If not specified, will use the token "
>         "generated when running `huggingface-cli login` (stored in `~/.huggingface`).",
>     )
>     parser.add_argument(
>         "--model_revision",
>         default="main",
>         type=str,
>         help="The specific model version to use (can be a branch name, tag name or commit id).",
>     )
>     parser.add_argument(
>         "--attn_softmax_bf16",
329c203,349
<         help="Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit",
---
>         help="Whether to run attention softmax layer in lower precision provided that the model supports it and "
>         "is also running in lower precision.",
>     )
>     parser.add_argument(
>         "--output_dir",
>         default=None,
>         type=str,
>         help="Output directory to store results in.",
>     )
>     parser.add_argument(
>         "--bucket_size",
>         default=-1,
>         type=int,
>         help="Bucket size to maintain static shapes. If this number is negative (default is -1) \
>             then we use `shape = prompt_length + max_new_tokens`. If a positive number is passed \
>             we increase the bucket in steps of `bucket_size` instead of allocating to max (`prompt_length + max_new_tokens`).",
>     )
>     parser.add_argument(
>         "--bucket_internal",
>         action="store_true",
>         help="Split kv sequence into buckets in decode phase. It improves throughput when max_new_tokens is large.",
>     )
>     parser.add_argument(
>         "--dataset_max_samples",
>         default=-1,
>         type=int,
>         help="If a negative number is passed (default = -1) perform inference on the whole dataset, else use only `dataset_max_samples` samples.",
>     )
>     parser.add_argument(
>         "--limit_hpu_graphs",
>         action="store_true",
>         help="Skip HPU Graph usage for first token to save memory",
>     )
>     parser.add_argument(
>         "--clear_hpu_graphs_cache",
>         action="store_true",
>         help="Clear HPU graphs cache",
>     )
>     parser.add_argument(
>         "--show_graphs_count",
>         action="store_true",
>         help="Show statistics of HPU graph compilation.",
>     )
>     parser.add_argument(
>         "--reuse_cache",
>         action="store_true",
>         help="Whether to reuse key/value cache for decoding. It should save memory.",
>     )
>     parser.add_argument("--verbose_workers", action="store_true", help="Enable output from non-master workers")
>     parser.add_argument(
>         "--simulate_dyn_prompt",
>         default=None,
>         type=int,
>         nargs="*",
>         help="If empty, static prompt is used. If a comma separated list of integers is passed, we warmup and use those shapes for prompt length.",
>     )
>     parser.add_argument(
>         "--reduce_recompile",
>         action="store_true",
>         help="Preprocess on cpu, and some other optimizations. Useful to prevent recompilations when using dynamic prompts (simulate_dyn_prompt)",
>     )
>     parser.add_argument(
>         "--use_chat_template",
>         action="store_true",
>         help="Wraps the prompt(s) in a chat template of `{ user: <prompt> }`",
>     )
>     parser.add_argument(
>         "--use_flash_attention",
>         action="store_true",
>         help="Whether to enable Habana Flash Attention, provided that the model supports it.",
>     )
>     parser.add_argument(
>         "--flash_attention_recompute",
>         action="store_true",
>         help="Whether to enable Habana Flash Attention in recompute mode on first token generation. This gives an opportunity of splitting graph internally which helps reduce memory consumption.",
>     )
>     parser.add_argument(
>         "--flash_attention_causal_mask",
>         action="store_true",
>         help="Whether to enable Habana Flash Attention in causal mode on first token generation.",
>     )
>     parser.add_argument(
>         "--flash_attention_fast_softmax",
>         nargs="?",
>         const=None,
>         action=SetTrueOrFalseOrNone,
>         help="Whether to enable Habana Flash Attention in fast softmax mode.",
>     )
>     parser.add_argument(
>         "--book_source",
>         action="store_true",
>         help="Whether to use project Guttenberg books data as input. Usefull for testing large sequence lengths.",
>     )
>     parser.add_argument(
>         "--torch_compile",
>         action="store_true",
>         help="Whether to use torch compiled model or not.",
>     )
>     parser.add_argument(
>         "--ignore_eos",
>         default=True,
>         action=argparse.BooleanOptionalAction,
>         help="Whether to disable stopping with eos token when calling `generate`. --no-ignore_eos to disable it",
>     )
>     parser.add_argument("--temperature", default=1.0, type=float, help="Temperature value for text generation")
>     parser.add_argument("--top_p", default=1.0, type=float, help="Top_p value for generating text via sampling")
>     parser.add_argument(
>         "--const_serialization_path",
>         "--csp",
>         type=str,
>         help="Path to serialize const params. Const params will be held on disk memory instead of being allocated on host memory.",
>     )
>     parser.add_argument(
>         "--trust_remote_code",
>         action="store_true",
>         help="Whether to trust the execution of code from datasets/models defined on the Hub. This option should only be set to `True` for repositories you trust and in which you have read the code, as it will execute code present on the Hub on your local machine.",
>     )
>     parser.add_argument(
>         "--parallel_strategy",
>         type=str,
>         choices=["tp", "ep", "none"],  # Add other strategies as needed
>         default="none",
>         help="Run multi card with the specified parallel strategy. Choices are 'tp' for Tensor Parallel Strategy or 'ep' for Expert Parallel Strategy or 'none'.",
>     )
>     parser.add_argument(
>         "--input_embeds",
>         action="store_true",
>         help="Whether to enable inputs_embeds or not.",
>     )
>     parser.add_argument(
>         "--run_partial_dataset",
>         action="store_true",
>         help="Run the inference with dataset for specified --n_iterations(default:5)",
>     )
>     parser.add_argument(
>         "--sdp_on_bf16", action="store_true", help="Allow pyTorch to use reduced precision in the SDPA math backend"
>     )
>     parser.add_argument(
>         "--save_quantized_model_with_inc",
>         action="store_true",
>         help="Save quantized Huggingface checkpoint using INC.",
>     )
>     parser.add_argument(
>         "--saved_model_path",
>         type=str,
>         default="inc_quantized_model",
>         help="A path to save quantized checkpoint.",
331,335d350
<     parser.add_argument("--jit", action="store_true", help="Whether or not to use jit trace to accelerate inference")
<     args = parser.parse_args()
< 
<     # Initialize the distributed state.
<     distributed_state = PartialState(cpu=args.use_cpu)
337c352,384
<     logger.warning(f"device: {distributed_state.device}, 16-bits inference: {args.fp16}")
---
>     quant_parser_group = parser.add_mutually_exclusive_group()
>     quant_parser_group.add_argument(
>         "--load_quantized_model_with_autogptq",
>         action="store_true",
>         help="Load an AutoGPTQ quantized checkpoint using AutoGPTQ.",
>     )
>     quant_parser_group.add_argument(
>         "--load_quantized_model_with_autoawq",
>         action="store_true",
>         help="Load an AutoAWQ quantized checkpoint using AutoAWQ.",
>     )
>     quant_parser_group.add_argument(
>         "--disk_offload",
>         action="store_true",
>         help="Whether to enable device map auto. In case no space left on cpu, weights will be offloaded to disk.",
>     )
>     quant_parser_group.add_argument(
>         "--load_quantized_model_with_inc",
>         action="store_true",
>         help="Load a quantized Huggingface checkpoint using INC.",
>     )
>     quant_parser_group.add_argument(
>         "--local_quantized_inc_model_path",
>         type=str,
>         default=None,
>         help="Path to neural-compressor quantized model, if set, the checkpoint will be loaded.",
>     )
>     parser.add_argument(
>         "--attn_batch_split",
>         default=1,
>         type=int,
>         help="Specify the batch size split for attention and mlp layers. 1 for no split. This is enabled only for prompt.",
>     )
339,340c386
<     if args.seed is not None:
<         set_seed(args.seed)
---
>     args = parser.parse_args()
342,369c388,389
<     # Initialize the model and tokenizer
<     try:
<         args.model_type = args.model_type.lower()
<         model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
<     except KeyError:
<         raise KeyError("the model {} you specified is not supported. You are welcome to add it and open a PR :)")
< 
<     tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)
<     if tokenizer.pad_token is None:
<         tokenizer.pad_token = tokenizer.eos_token
<     model = model_class.from_pretrained(args.model_name_or_path)
< 
<     # Set the model to the right device
<     model.to(distributed_state.device)
< 
<     if args.fp16:
<         model.half()
<     max_seq_length = getattr(model.config, "max_position_embeddings", 0)
<     args.length = adjust_length_to_model(args.length, max_sequence_length=max_seq_length)
<     logger.info(args)
< 
<     prompt_text = args.prompt if args.prompt else input("Model prompt >>> ")
< 
<     # Different models need different input formatting and/or extra arguments
<     requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()
<     if requires_preprocessing:
<         prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)
<         preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)
---
>     if args.torch_compile:
>         args.use_hpu_graphs = False
371,374c391,392
<         if model.__class__.__name__ in ["TransfoXLLMHeadModel"]:
<             tokenizer_kwargs = {"add_space_before_punct_symbol": True}
<         else:
<             tokenizer_kwargs = {}
---
>     if not args.use_hpu_graphs:
>         args.limit_hpu_graphs = False
376,377c394,396
<         encoded_prompt = tokenizer.encode(
<             preprocessed_prompt_text, add_special_tokens=False, return_tensors="pt", **tokenizer_kwargs
---
>     if args.use_flash_attention and args.flash_attention_fast_softmax is None:
>         logger.warning(
>             "`--flash_attention_fast_softmax` was not set; defaulting to True due to `--use_flash_attention` being enabled."
378a398
>         args.flash_attention_fast_softmax = True
380,382c400
<         prefix = args.prefix if args.prefix else args.padding_text
<         encoded_prompt = tokenizer.encode(prefix + prompt_text, add_special_tokens=False, return_tensors="pt")
<     encoded_prompt = encoded_prompt.to(distributed_state.device)
---
>         args.flash_attention_fast_softmax = False
384,385c402,668
<     if encoded_prompt.size()[-1] == 0:
<         input_ids = None
---
>     args.quant_config = os.getenv("QUANT_CONFIG", "")
>     if args.quant_config and args.load_quantized_model_with_autogptq:
>         raise RuntimeError("Setting both quant_config and load_quantized_model_with_autogptq is unsupported. ")
>     if args.quant_config and args.load_quantized_model_with_autoawq:
>         raise RuntimeError("Setting both quant_config and load_quantized_model_with_autoawq is unsupported. ")
> 
>     if args.quant_config == "" and args.disk_offload:
>         logger.warning(
>             "`--disk_offload` was tested only with fp8, it may not work with full precision. If error raises try to remove the --disk_offload flag."
>         )
>     return args
> 
> 
> def prepare_generation_embedding(model, model_name, input_tokens):
>     batch_size = input_tokens["input_ids"].size(0)
> 
>     inputs_embeds = model.get_input_embeddings()(input_tokens["input_ids"])
> 
>     if inputs_embeds.size(0) != batch_size:
>         inputs_embeds = inputs_embeds.expand(batch_size, -1, -1)
> 
>     attention_mask = input_tokens["attention_mask"]
>     return {"inputs_embeds": inputs_embeds, "attention_mask": attention_mask}
> 
> 
> def main():
>     parser = argparse.ArgumentParser()
>     args = setup_parser(parser)
>     model, assistant_model, tokenizer, generation_config = initialize_model(args, logger)
> 
>     use_lazy_mode = True
>     if args.torch_compile:
>         use_lazy_mode = False
> 
>     import habana_frameworks.torch.hpu as torch_hpu
> 
>     if args.sdp_on_bf16:
>         torch._C._set_math_sdp_allow_fp16_bf16_reduction(True)
> 
>     if args.dataset_name is None:
>         # Benchmark over the prompts below
>         if args.prompt:
>             input_sentences = args.prompt
>         elif args.book_source:
> 
>             def download_book(book_id):
>                 import os
> 
>                 import requests
> 
>                 url = f"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt"
>                 response = requests.get(url)
>                 if response.status_code == 200:
>                     pid = os.getpid()
>                     save_path = f"/tmp/{book_id}_{pid}.txt"
>                     with open(save_path, "wb") as file:
>                         file.write(response.content)
>                     print(f"Book downloaded and saved to: {save_path}")
>                     return save_path
>                 else:
>                     print("Failed to download book! Exiting...")
>                     import sys
> 
>                     sys.exit()
> 
>             def assemble_prompt(prompt_size, book_path):
>                 prompt = ""
>                 counter = 0
>                 book_lines = open(book_path).readlines()
>                 for line in book_lines:
>                     for word in line.split():
>                         counter += 1
>                         prompt += word + " "
>                         if counter == prompt_size:
>                             return [prompt] * args.batch_size
> 
>             book_ids = [
>                 2701,  # Moby Dick; Or, The Whale
>                 1513,  # Romeo and Juliet
>                 1342,  # Pride and Prejudice
>             ]
>             input_sentences = assemble_prompt(prompt_size=args.max_input_tokens, book_path=download_book(book_ids[0]))
>         else:
>             input_sentences = [
>                 "DeepSpeed is a machine learning framework",
>                 "He is working on",
>                 "He has a",
>                 "He got all",
>                 "Everyone is happy and I can",
>                 "The new movie that got Oscar this year",
>                 "In the far far distance from our galaxy,",
>                 "Peace is the only way",
>             ]
> 
>         if args.batch_size > len(input_sentences):
>             # Dynamically extends to support larger batch sizes
>             num_sentences_to_add = args.batch_size - len(input_sentences)
>             for i in range(num_sentences_to_add):
>                 input_sentences.append(input_sentences[i % len(input_sentences)])
>         elif args.batch_size < len(input_sentences):
>             input_sentences = input_sentences[: args.batch_size]
> 
>         def generate(size=None, reduce_recompile=False):
>             """Generates sequences from the input sentences and returns them."""
>             encode_t0 = time.perf_counter()
>             # Tokenization
>             if args.max_input_tokens > 0:
>                 if hasattr(model.config, "type_vocab_size") and model.config.type_vocab_size > 0:
>                     return_token_type_ids = True
>                 else:
>                     return_token_type_ids = False
> 
>                 input_tokens = tokenizer.batch_encode_plus(
>                     input_sentences,
>                     return_tensors="pt",
>                     padding="max_length",
>                     max_length=args.max_input_tokens,
>                     truncation=True,
>                     return_token_type_ids=return_token_type_ids,
>                 )
> 
>                 def compute_valid_sequence_lengths_tensor(input_tokens):
>                     attn_mask = input_tokens["attention_mask"]
>                     return torch.sum(attn_mask, dim=1, dtype=torch.int32)
> 
>                 valid_sequence_lengths = compute_valid_sequence_lengths_tensor(input_tokens).to(args.device)
>                 generation_config.valid_sequence_lengths = valid_sequence_lengths
>             elif args.use_chat_template:
>                 input_messages = [{"role": "user", "content": sentence} for sentence in input_sentences]
>                 input_ids = tokenizer.apply_chat_template(input_messages, return_tensors="pt", padding=True)
>                 attention_mask = torch.ones_like(input_ids)
>                 input_tokens = BatchEncoding({"input_ids": input_ids, "attention_mask": attention_mask})
>             else:
>                 input_tokens = tokenizer.batch_encode_plus(input_sentences, return_tensors="pt", padding=True)
>             encode_duration = time.perf_counter() - encode_t0
> 
>             if size is not None:
>                 input_tokens = adjust_batch(input_tokens, size)
>             if not reduce_recompile:
>                 # Move inputs to target device(s)
>                 for t in input_tokens:
>                     if torch.is_tensor(input_tokens[t]):
>                         input_tokens[t] = input_tokens[t].to(args.device)
> 
>             input_data = {}
>             if args.input_embeds:
>                 inputs_embeds = prepare_generation_embedding(model, args.model_name_or_path, input_tokens)
>                 if inputs_embeds is not None:
>                     input_data.update(inputs_embeds)
>                     input_data.update(input_tokens)
>                 else:
>                     args.input_embeds = False
>                     input_data.update(input_tokens)
>             else:
>                 input_data.update(input_tokens)
> 
>             iteration_times = []
>             outputs = model.generate(
>                 **input_data,
>                 generation_config=generation_config,
>                 assistant_model=assistant_model,
>                 lazy_mode=use_lazy_mode,
>                 hpu_graphs=args.use_hpu_graphs,
>                 profiling_steps=args.profiling_steps,
>                 profiling_warmup_steps=args.profiling_warmup_steps,
>                 ignore_eos=args.ignore_eos,
>                 iteration_times=iteration_times,
>                 profiling_record_shapes=args.profiling_record_shapes,
>             ).cpu()
>             first_token_time = iteration_times[0] + encode_duration
>             logger.info(f"Time to first token = {first_token_time * 1000}ms")
>             return tokenizer.batch_decode(outputs, skip_special_tokens=True)
> 
>         from optimum.habana.utils import HabanaProfile
> 
>         # compilation stage disable profiling
>         HabanaProfile.disable()
>         # Compilation
>         logger.info("Graph compilation...")
>         dyn_prompt_lens = args.simulate_dyn_prompt
>         t0 = time.perf_counter()
>         # The first three iterations take longer because of graph compilation
>         if dyn_prompt_lens is None or len(set(dyn_prompt_lens)) == 1:
>             for i in range(args.warmup):
>                 if dyn_prompt_lens is None:
>                     print(f"Warming up iteration {i + 1}/{args.warmup}", flush=True)
>                     generate(None, args.reduce_recompile)
>                 else:
>                     print(f"Warming up for shape {dyn_prompt_lens[0]} iteration {i + 1}/{args.warmup}", flush=True)
>                     generate(dyn_prompt_lens[0], args.reduce_recompile)
>         else:
>             if args.bucket_size > 0:
>                 mn = min(dyn_prompt_lens)
>                 mx = max(dyn_prompt_lens)
> 
>                 def rounder(x):
>                     return int(math.ceil(x / args.bucket_size) * args.bucket_size)
> 
>                 min_prompt_len = rounder(mn)
>                 max_sentence_len = rounder(mx)
>                 for i in range(args.warmup):
>                     lst = list(range(min_prompt_len, max_sentence_len + 1, args.bucket_size))
>                     for sz in lst:
>                         print(f"Warming up for shape {sz - 1} iteration {i + 1}/{args.warmup}", flush=True)
>                         generate(sz - 1, args.reduce_recompile)
>         torch_hpu.synchronize()
>         compilation_duration = time.perf_counter() - t0
>         HabanaProfile.enable()
>         total_new_tokens_generated = 0
>         logger.info("Running generate...")
>         t0 = time.perf_counter()
>         # Benchmark over n_iterations iterations
>         if dyn_prompt_lens is None:
>             for i in range(args.n_iterations):
>                 generated = generate(None, args.reduce_recompile)
>         else:
>             repeated_prompt_len = cycle(dyn_prompt_lens)
>             for i in range(args.n_iterations):
>                 prompt_len = next(repeated_prompt_len)
>                 print("Generating for shape,", prompt_len)
>                 generated = generate(prompt_len, args.reduce_recompile)
>         duration = time.perf_counter() - t0
>         total_new_tokens_generated = args.n_iterations * args.batch_size * args.max_new_tokens
>         throughput = total_new_tokens_generated / duration
> 
>         print()
>         print("Input/outputs:")
>         all_inputs = []
>         all_outputs = []
>         for i, input_sentence in enumerate(zip(input_sentences)):
>             print(f"input {i + 1}: {input_sentence}")
>             all_inputs.append(input_sentence)
>             for j, output in enumerate(
>                 zip(generated[args.num_return_sequences * i : args.num_return_sequences * (i + 1)])
>             ):
>                 print(f"output {i + 1}.{j + 1}: {output}")
>                 all_outputs.append(output)
>             print()
> 
>         # Store results if necessary
>         if args.output_dir is not None and args.global_rank == 0:
>             output_dir = Path(args.output_dir)
>             output_dir.mkdir(parents=True, exist_ok=True)
> 
>             results = {
>                 "throughput": throughput,
>                 "input": all_inputs,
>                 "output": all_outputs,
>             }
>             with (output_dir / "results.json").open("w", encoding="utf-8") as f:
>                 json.dump(results, f, ensure_ascii=False, indent=4)
> 
>         stats = "Input embeds" if args.input_embeds else "Input tokens"
>         stats = stats + f"\nThroughput (including tokenization) = {throughput} tokens/second"
>         if args.show_graphs_count:
>             stats = stats + f"\nNumber of HPU graphs                = {count_hpu_graphs()}"
>         separator = "-" * len(stats)
>         print()
>         print("Stats:")
>         print(separator)
>         print(stats)
>         mem = get_hpu_memory_stats()
>         for k, v in mem.items():
>             print("{:35} = {} GB".format(k[:-5].replace("_", " ").capitalize(), v))
>         print(f"Graph compilation duration          = {compilation_duration} seconds")
>         print(separator)
>         print()
387c670,687
<         input_ids = encoded_prompt
---
>         # Downloading and loading a dataset from the hub.
>         from datasets import load_dataset
>         from torch.utils.data import DataLoader
> 
>         assert not args.simulate_dyn_prompt, "Both dataset_name and simulate_dyn_prompt are set"
> 
>         raw_dataset = load_dataset(args.dataset_name, trust_remote_code=args.trust_remote_code)
>         if "test" in raw_dataset:
>             split = "test"
>         elif "validation" in raw_dataset:
>             split = "validation"
>         else:
>             split = "train"
>         raw_dataset = (
>             raw_dataset[split]
>             .shuffle()
>             .select(range(args.dataset_max_samples if args.dataset_max_samples > 0 else (raw_dataset[split]).num_rows))
>         )
389,395c689,696
<     if args.jit:
<         jit_input_texts = ["enable jit"]
<         jit_inputs = prepare_jit_inputs(jit_input_texts, model, tokenizer)
<         torch._C._jit_set_texpr_fuser_enabled(False)
<         model.config.return_dict = False
<         if hasattr(model, "forward"):
<             sig = inspect.signature(model.forward)
---
>         if args.column_name is None:
>             # If no column name is given, take the first column that has strings
>             column_name = [key for key in raw_dataset.features.keys() if raw_dataset.features[key].dtype == "string"][
>                 0
>             ]
>             logger.info(
>                 f"No column name was given so automatically choosing '{column_name}' for prompts. If you would like to use another column of the dataset, you can set the argument `--column_name`."
>             )
397,435c698,718
<             sig = inspect.signature(model.__call__)
<         jit_inputs = tuple(jit_inputs[key] for key in sig.parameters if jit_inputs.get(key, None) is not None)
<         traced_model = torch.jit.trace(model, jit_inputs, strict=False)
<         traced_model = torch.jit.freeze(traced_model.eval())
<         traced_model(*jit_inputs)
<         traced_model(*jit_inputs)
< 
<         model = _ModelFallbackWrapper(traced_model, model)
< 
<     output_sequences = model.generate(
<         input_ids=input_ids,
<         max_length=args.length + len(encoded_prompt[0]),
<         temperature=args.temperature,
<         top_k=args.k,
<         top_p=args.p,
<         repetition_penalty=args.repetition_penalty,
<         do_sample=True,
<         num_return_sequences=args.num_return_sequences,
<     )
< 
<     # Remove the batch dimension when returning multiple sequences
<     if len(output_sequences.shape) > 2:
<         output_sequences.squeeze_()
< 
<     generated_sequences = []
< 
<     for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
<         print(f"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===")
<         generated_sequence = generated_sequence.tolist()
< 
<         # Decode text
<         text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)
< 
<         # Remove all text after the stop token
<         text = text[: text.find(args.stop_token) if args.stop_token else None]
< 
<         # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing
<         total_sequence = (
<             prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]
---
>             column_name = args.column_name
> 
>         # Remove unused columns
>         raw_dataset = raw_dataset.remove_columns([name for name in raw_dataset.column_names if name != column_name])
> 
>         # Set the prompt length to args.max_input_tokens if > 0 else (if 0 truncate to 16, otherwise use full length)
>         prompt_length = args.max_input_tokens if args.max_input_tokens > 0 else (-1, 16)[args.max_input_tokens == 0]
> 
>         def preprocess_function(examples):
>             # Tokenize the texts
>             return tokenizer(
>                 examples[column_name],
>                 padding="max_length",
>                 max_length=prompt_length if prompt_length > 0 else None,
>                 truncation=prompt_length > 0,
>             )
> 
>         raw_dataset = raw_dataset.map(
>             preprocess_function,
>             batched=True,
>             desc="Running tokenizer on dataset",
436a720,743
>         # After tokenization, we can remove the column of interest
>         raw_dataset = raw_dataset.remove_columns([column_name])
>         raw_dataset.set_format(type="torch")
> 
>         if prompt_length <= 0:
>             # Todo please check if this collate function is suitable for your model
>             # This has been tested for OPT, llama, and Bloom
>             assert model.config.model_type in ["opt", "bloom", "llama"]
> 
>             def collate_fn(data):
>                 collect = {k: [dt[k] for dt in data] for k in data[0]}
>                 result = {}
>                 for k in collect:
>                     tensors = collect[k]
>                     max_shape = max([item.shape[0] for item in tensors])
>                     result[k] = torch.stack(
>                         [torch.cat((torch.zeros(max_shape - t.shape[0], dtype=t.dtype), t)) for t in tensors], 0
>                     )
>                 return result
> 
>         else:
>             collate_fn = None
> 
>         dataloader = DataLoader(raw_dataset, batch_size=args.batch_size, collate_fn=collate_fn)
438,439c745,822
<         generated_sequences.append(total_sequence)
<         print(total_sequence)
---
>         def generate_dataset(batch):
>             prompt = tokenizer.batch_decode(batch["input_ids"], skip_special_tokens=True)
>             # Move inputs to target device(s)
>             for t in batch:
>                 if torch.is_tensor(batch[t]):
>                     batch[t] = batch[t].to(args.device)
>             # Generate new sequences
>             outputs = model.generate(
>                 **batch,
>                 generation_config=generation_config,
>                 lazy_mode=use_lazy_mode,
>                 hpu_graphs=args.use_hpu_graphs,
>                 profiling_steps=args.profiling_steps,
>                 profiling_warmup_steps=args.profiling_warmup_steps,
>                 ignore_eos=args.ignore_eos,
>                 profiling_record_shapes=args.profiling_record_shapes,
>             ).cpu()
>             return prompt, outputs
> 
>         # warmup
>         from optimum.habana.utils import HabanaProfile
> 
>         # compilation stage disable profiling
>         HabanaProfile.disable()
>         # Compilation
>         logger.info("Graph compilation...")
>         t0 = time.perf_counter()
>         for i, batch in enumerate(dataloader):
>             generate_dataset(batch)
>             # The first three iterations take longer because of graph compilation
>             if (i + 1) == 3:
>                 break
>         torch_hpu.synchronize()
>         compilation_duration = time.perf_counter() - t0
>         HabanaProfile.enable()
> 
>         total_new_tokens_generated = 0
>         duration = 0
>         separator = "-" * 50
>         logger.info("Running generate dataset...")
>         t_start = time.time()
>         for i, batch in enumerate(dataloader):
>             t0 = time.perf_counter()
>             prompt, outputs = generate_dataset(batch)
>             duration += time.perf_counter() - t0
>             total_new_tokens_generated += args.batch_size * args.max_new_tokens
>             print(separator)
>             print(f"Batch nÂ°{i + 1}")
>             print(f"Input: {prompt[: args.batch_size]}")
>             print(
>                 f"Output: {tokenizer.batch_decode(outputs, skip_special_tokens=True)[: args.batch_size * args.num_return_sequences]}"
>             )
>             print(separator)
>             if args.run_partial_dataset and args.n_iterations == i + 1:
>                 break
>         t_end = time.time()
> 
>         throughput = total_new_tokens_generated / duration
>         # Print Stats
> 
>         stats = f"Throughput (including tokenization) = {throughput} tokens/second"
>         separator = "-" * len(stats)
>         print()
>         print("Stats:")
>         print(separator)
>         print(stats)
>         print("Total runtime for dataset:", t_end - t_start)
>         mem = get_hpu_memory_stats()
>         for k, v in mem.items():
>             print("{:35} = {} GB".format(k[:-5].replace("_", " ").capitalize(), v))
>         print(f"Graph compilation duration          = {compilation_duration} seconds")
>         print(separator)
>     if args.quant_config:
>         finalize_quantization(model)
>     if args.save_quantized_model_with_inc:
>         save_model(model, tokenizer, args.saved_model_path)
>     if args.const_serialization_path and os.path.isdir(args.const_serialization_path):
>         import shutil
441c824
<     return generated_sequences
---
>         shutil.rmtree(args.const_serialization_path)
