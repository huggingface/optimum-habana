{
    "gaudi": {
        "tatsu-lab/alpaca": {
            "num_train_epochs": 1,
            "eval_batch_size": 2,
            "distribution": {
                "multi_card": {
                    "learning_rate": 1e-4,
                    "train_batch_size": 2,
                    "perplexity": 2.7542,
                    "train_runtime": 538.0159,
                    "train_samples_per_second": 20.397,
                    "extra_arguments": [
                        "--bf16",
                        "--gradient_accumulation_steps 4",
                        "--save_strategy no",
                        "--use_hpu_graphs_for_inference",
                        "--dataset_concatenation",
                        "--validation_split_percentage 10",
                        "--max_steps 100",
                        "--attn_softmax_bf16"
                    ]
                }
            }
        }
    },
    "gaudi2": {
        "tatsu-lab/alpaca": {
            "num_train_epochs": 3,
            "eval_batch_size": 4,
            "distribution": {
                "multi_card": {
                    "learning_rate": 3e-4,
                    "train_batch_size": 8,
                    "perplexity": 2.3665,
                    "train_runtime": 294.5707,
                    "train_samples_per_second": 148.093,
                    "extra_arguments": [
                        "--bf16",
                        "--gradient_accumulation_steps 2",
                        "--evaluation_strategy no",
                        "--save_strategy no",
                        "--warmup_ratio  0.03",
                        "--lr_scheduler_type constant",
                        "--max_grad_norm  0.3",
                        "--logging_steps 1",
                        "--use_hpu_graphs_for_inference",
                        "--lora_rank 8",
                        "--lora_alpha 16",
                        "--lora_dropout 0.05",
                        "--lora_target_modules q_proj v_proj",
                        "--dataset_concatenation",
                        "--max_seq_length 512",
                        "--low_cpu_mem_usage True",
                        "--adam_epsilon 1e-08",
                        "--ddp_bucket_cap_mb 50",
                        "--validation_split_percentage 10",
                        "--attn_softmax_bf16"
                    ]
                }
            }
        },
        "tatsu-lab/alpaca_fsdpcompile": {
            "num_train_epochs": 1,
            "eval_batch_size": 1,
            "distribution": {
                "multi_card": {
                    "learning_rate": 3e-4,
                    "train_batch_size": 8,
                    "perplexity": 2.4259,
                    "train_runtime": 186.2483,
                    "train_samples_per_second": 93.5,
                    "extra_arguments": [
                        "--bf16 True",
                        "--gradient_accumulation_steps 2",
                        "--evaluation_strategy no",
                        "--save_strategy no",
                        "--warmup_ratio  0.03",
                        "--lr_scheduler_type constant",
                        "--max_grad_norm  0.3",
                        "--logging_steps 1",
                        "--lora_rank 8",
                        "--lora_alpha 16",
                        "--lora_dropout 0.05",
                        "--lora_target_modules q_proj v_proj",
                        "--dataset_concatenation",
                        "--max_seq_length 512",
                        "--low_cpu_mem_usage True",
                        "--adam_epsilon 1e-08",
                        "--ddp_bucket_cap_mb 50",
                        "--validation_split_percentage 10",
                        "--attn_softmax_bf16",
                        "--pipelining_fwd_bwd False",
                        "--fsdp auto_wrap",
                        "--torch_compile_backend hpu_backend",
                        "--torch_compile",
                        "--fsdp_config examples/language-modeling/fsdp_config.json"
                    ]
                }
            }
        },
        "trl-sft": {
            "num_train_epochs": 1,
            "eval_batch_size": 1,
            "distribution": {
                "multi_card": {
                    "learning_rate": 1e-4,
                    "train_batch_size": 4,
                    "train_runtime": 206,
                    "train_samples_per_second": 51.54,
                    "extra_arguments": [
                        "--bf16 True",
                        "--gradient_accumulation_steps 2",
                        "--evaluation_strategy no",
                        "--save_strategy no",
                        "--warmup_ratio  0.03",
                        "--lr_scheduler_type constant",
                        "--max_grad_norm  0.3",
                        "--logging_steps 1",
                        "--lora_r 8",
                        "--lora_alpha 16",
                        "--lora_dropout 0.05",
                        "--lora_target_modules q_proj v_proj",
                        "--seq_length 1024",
                        "--optim paged_adamw_32bit",
                        "--weight_decay 0.05",
                        "--report_to none",
                        "--max_steps 100"
                    ]
                }
            }
        },
        "trl-dpo": {
            "num_train_epochs": 1,
            "eval_batch_size": 1,
            "distribution": {
                "multi_card": {
                    "learning_rate": 5e-4,
                    "train_batch_size": 1,
                    "train_runtime": 234.6471,
                    "train_samples_per_second": 13.499,
                    "extra_arguments": [
                        "--logging_steps 1",
                        "--lora_r 8",
                        "--lora_alpha 16",
                        "--lora_dropout 0.05",
                        "--lora_target_modules q_proj v_proj k_proj out_proj fc_in fc_out wte",
                        "--max_length 1024",
                        "--max_prompt_length 512",
                        "--report_to none",
                        "--max_steps 100",
                        "--eval_steps 200",
                        "--lr_scheduler_type cosine",
                        "--warmup_steps 0",
                        "--weight_decay 0.05",
                        "--optimizer_type paged_adamw_32bit",
                        "--beta 0.1",
                        "--gradient_accumulation_steps 4",
                        "--sanity_check"
                    ]
                }
            }
        }
    }
}