{
  "tests/test_examples.py::CausalLanguageModelingExampleTester::test_run_clm_gemma-2b-it_single_card": {
    "gaudi2": {
      "perplexity": 26.39,
      "train_runtime": 356.07,
      "train_samples_per_second": 14.06
    },
    "gaudi3": {
      "perplexity": 26.271165167474585,
      "train_runtime": 218.4737,
      "train_samples_per_second": 23.781
    }
  },
  "tests/test_examples.py::CausalLanguageModelingLORAExampleTester::test_run_lora_clm_llama-7b_single_card": {
    "gaudi1": {
      "perplexity": 3.9168,
      "train_runtime": 132.665,
      "train_samples_per_second": 2.295
    },
    "gaudi2": {
      "perplexity": 3.8436,
      "train_runtime": 113.9713,
      "train_samples_per_second": 18.428
    },
    "gaudi3": {
      "perplexity": 3.843924462719278,
      "train_runtime": 148.7151,
      "train_samples_per_second": 29.824
    }
  },
  "tests/test_examples.py::DeepSpeedTextClassificationExampleTester::test_run_glue_LlamaGuard-7b_deepspeed": {
    "gaudi2": {
      "eval_f1": 0.8811,
      "train_runtime": 68.1838,
      "train_samples_per_second": 342.169
    },
    "gaudi3": {
      "eval_f1": 0.8809523809523809,
      "train_runtime": 232.6707,
      "train_samples_per_second": 560.75
    }
  },
  "tests/test_examples.py::DeepspeedCausalLanguageModelingExampleTester::test_run_clm_CodeLlama-13b-Instruct-hf_deepspeed": {
    "gaudi2": {
      "perplexity": 6.877496628184696,
      "train_runtime": 542.2985,
      "train_samples_per_second": 18.789
    },
    "gaudi3": {
      "perplexity": 6.877100646486551,
      "train_runtime": 519.1738,
      "train_samples_per_second": 29.814
    }
  },
  "tests/test_examples.py::DeepspeedCausalLanguageModelingExampleTester::test_run_clm_bloom-7b1_deepspeed": {
    "gaudi1": {
      "train_runtime": 1556.481,
      "train_samples_per_second": 4.757
    }
  },
  "tests/test_examples.py::DeepspeedCausalLanguageModelingExampleTester::test_run_clm_chatglm3-6b_deepspeed": {
    "gaudi2": {
      "perplexity": 16.51629,
      "train_runtime": 445,
      "train_samples_per_second": 18.216
    },
    "gaudi3": {
      "perplexity": 16.260238201071928,
      "train_runtime": 375.5908,
      "train_samples_per_second": 21.888
    }
  },
  "tests/test_examples.py::DeepspeedCausalLanguageModelingExampleTester::test_run_clm_gemma-2b-it_deepspeed": {
    "gaudi2": {
      "perplexity": 12.8,
      "train_runtime": 80.3429,
      "train_samples_per_second": 76.06
    },
    "gaudi3": {
      "perplexity": 12.8,
      "train_runtime": 57.7676,
      "train_samples_per_second": 135.39
    }
  },
  "tests/test_examples.py::DeepspeedCausalLanguageModelingExampleTester::test_run_clm_gpt-neox-20b_deepspeed": {
    "gaudi2": {
      "perplexity": 8.169664686471043,
      "train_runtime": 445,
      "train_samples_per_second": 7.328
    },
    "gaudi3": {
      "perplexity": 7.827201417363628,
      "train_runtime": 445.3031,
      "train_samples_per_second": 11.704
    }
  },
  "tests/test_examples.py::DeepspeedCausalLanguageModelingExampleTester::test_run_clm_gpt2-xl_deepspeed": {
    "gaudi1": {
      "perplexity": 12.6744,
      "train_runtime": 366.8694,
      "train_samples_per_second": 16.464
    },
    "gaudi2": {
      "perplexity": 13.1786,
      "train_runtime": 241.093,
      "train_samples_per_second": 96.789
    },
    "gaudi3": {
      "perplexity": 13.143,
      "train_runtime": 198.1031,
      "train_samples_per_second": 147.801
    }
  },
  "tests/test_examples.py::DeepspeedSFTExampleTester::test_sft_Qwen2-72B_deepspeed": {
    "gaudi2": {
      "perplexity": 3.7020898897918824,
      "train_runtime": 918.8018,
      "train_samples_per_second": 7.554
    },
    "gaudi3": {
      "perplexity": 3.728595328528421,
      "train_runtime": 712.4347,
      "train_samples_per_second": 13.065
    }
  },
  "tests/test_examples.py::DeepspeedSummarizationExampleTester::test_run_summarization_flan-t5-xxl_deepspeed": {
    "gaudi2": {
      "eval_rougeLsum": 27.9095,
      "train_runtime": 141.557,
      "train_samples_per_second": 32.239
    },
    "gaudi3": {
      "eval_rougeLsum": 28.0738,
      "train_runtime": 137.2073,
      "train_samples_per_second": 44.762
    }
  },
  "tests/test_examples.py::EagerModeCausalLanguageModelingExampleTester::test_run_clm_gemma-2b-it_single_card": {
    "gaudi2": {
      "perplexity": 26.69,
      "train_runtime": 560.8188,
      "train_samples_per_second": 8.597
    },
    "gaudi3": {
      "perplexity": 26.299428898047232,
      "train_runtime": 318.8908,
      "train_samples_per_second": 15.166
    }
  },
  "tests/test_examples.py::ImageClassificationExampleTester::test_run_image_classification_swin-base-patch4-window7-224-in22k_single_card": {
    "gaudi1": {
      "eval_accuracy": 0.9871,
      "train_runtime": 246.4134,
      "train_samples_per_second": 212.722
    },
    "gaudi2": {
      "eval_accuracy": 0.9871,
      "train_runtime": 74.7251,
      "train_samples_per_second": 831.235
    },
    "gaudi3": {
      "eval_accuracy": 0.9849333333333333,
      "train_runtime": 86.4819,
      "train_samples_per_second": 949.371
    }
  },
  "tests/test_examples.py::ImageClassificationExampleTester::test_run_image_classification_vit-base-patch16-224-in21k_single_card": {
    "gaudi1": {
      "eval_accuracy": 0.9812,
      "train_runtime": 136.9418,
      "train_samples_per_second": 359.584
    },
    "gaudi2": {
      "eval_accuracy": 0.9696,
      "train_runtime": 54.0092,
      "train_samples_per_second": 886.623
    },
    "gaudi3": {
      "eval_accuracy": 0.9690666666666666,
      "train_runtime": 51.3575,
      "train_samples_per_second": 1100.39
    }
  },
  "tests/test_examples.py::MultiCardAudioClassificationExampleTester::test_run_audio_classification_ast-finetuned-speech-commands-v2_multi_card": {
    "gaudi2": {
      "eval_accuracy": 0.1871,
      "eval_samples_per_second": 2301.088,
      "train_runtime": 139.9477,
      "train_samples_per_second": 1955.74
    },
    "gaudi3": {
      "eval_accuracy": 0.18546195652173914,
      "eval_samples_per_second": 2442.221,
      "train_runtime": 118.0048,
      "train_samples_per_second": 2462.94
    }
  },
  "tests/test_examples.py::MultiCardAudioClassificationExampleTester::test_run_audio_classification_wav2vec2-base_multi_card": {
    "gaudi1": {
      "eval_accuracy": 0.8013,
      "eval_samples_per_second": 329.12,
      "train_runtime": 366.8081,
      "train_samples_per_second": 716.385
    },
    "gaudi2": {
      "eval_accuracy": 0.7228,
      "eval_samples_per_second": 3640.021,
      "train_runtime": 63.4079,
      "train_samples_per_second": 2975.844
    },
    "gaudi3": {
      "eval_accuracy": 0.7202785326086957,
      "eval_samples_per_second": 2059.992,
      "train_runtime": 71.6177,
      "train_samples_per_second": 3977.496
    }
  },
  "tests/test_examples.py::MultiCardBridgetowerExampleTester::test_run_bridgetower_bridgetower-large-itm-mlm-itc_multi_card": {
    "gaudi2": {
      "train_runtime": 224.42,
      "train_samples_per_second": 904.93
    },
    "gaudi3": {
      "train_runtime": 496.3196,
      "train_samples_per_second": 783.481
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingAdaloraExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 2.59,
      "train_runtime": 459,
      "train_samples_per_second": 107
    },
    "gaudi3": {
      "perplexity": 2.631517742463695,
      "train_runtime": 1026.9536,
      "train_samples_per_second": 71.308
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingExampleTester::test_run_clm_gemma-2b-it_multi_card": {
    "gaudi2": {
      "perplexity": 12.8,
      "train_runtime": 82.6617,
      "train_samples_per_second": 94.524
    },
    "gaudi3": {
      "perplexity": 12.8,
      "train_runtime": 66.2529,
      "train_samples_per_second": 159.47
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingIA3ExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 3.3,
      "train_runtime": 262.8,
      "train_samples_per_second": 161
    },
    "gaudi3": {
      "perplexity": 3.291398111098924,
      "train_runtime": 485.2917,
      "train_samples_per_second": 233.759
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLORAExampleTester2::test_run_lora_clm_falcon-40b_multi_card": {
    "gaudi2": {
      "perplexity": 1.6,
      "train_runtime": 710,
      "train_samples_per_second": 15.0
    },
    "gaudi3": {
      "perplexity": 1.588740773299791,
      "train_runtime": 574.862,
      "train_samples_per_second": 24.276
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLORAExampleTester2::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 2.3665,
      "train_runtime": 294.5707,
      "train_samples_per_second": 148.093
    },
    "gaudi3": {
      "perplexity": 1.570946503005108,
      "train_runtime": 418.2808,
      "train_samples_per_second": 238.235
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLORAExampleTester::test_run_lora_clm_falcon-40b_multi_card": {
    "gaudi2": {
      "perplexity": 4.0,
      "train_runtime": 550,
      "train_samples_per_second": 15.0
    },
    "gaudi3": {
      "perplexity": 3.694849124063941,
      "train_runtime": 463.3435,
      "train_samples_per_second": 24.637
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLORAExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi1": {
      "perplexity": 2.7542,
      "train_runtime": 538.0159,
      "train_samples_per_second": 20.397
    },
    "gaudi2": {
      "perplexity": 2.3665,
      "train_runtime": 294.5707,
      "train_samples_per_second": 148.093
    },
    "gaudi3": {
      "perplexity": 2.3665888138128466,
      "train_runtime": 442.8761,
      "train_samples_per_second": 219.733
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLORAFSDPCompileExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 2.4259,
      "train_runtime": 186.2483,
      "train_samples_per_second": 93.5
    },
    "gaudi3": {
      "perplexity": 2.42632366178759,
      "train_runtime": 166.8724,
      "train_samples_per_second": 73.249
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLlamaAdapterExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 5.575,
      "train_runtime": 131.7,
      "train_samples_per_second": 294
    },
    "gaudi3": {
      "perplexity": 5.575957971980852,
      "train_runtime": 268.6561,
      "train_samples_per_second": 456.126
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLnExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 2.83,
      "train_runtime": 249,
      "train_samples_per_second": 165
    },
    "gaudi3": {
      "perplexity": 2.842264808115683,
      "train_runtime": 407.6245,
      "train_samples_per_second": 240.47
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLoRACPExampleTester::test_run_lora_clm_llama-7b_deepspeed": {
    "gaudi2": {
      "perplexity": 2.8889,
      "train_runtime": 147.3597,
      "train_samples_per_second": 34.41
    },
    "gaudi3": {
      "perplexity": 2.8421374130082477,
      "train_runtime": 268.0088,
      "train_samples_per_second": 52.488
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingLoRAFP8ExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 2.3692,
      "train_runtime": 411.9935,
      "train_samples_per_second": 232.439
    },
    "gaudi3": {
      "perplexity": 2.3750491436810424,
      "train_runtime": 670.2323,
      "train_samples_per_second": 323.175
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingPTuningExampleTester::test_run_prompt_tuning_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 1.047,
      "train_runtime": 18.7,
      "train_samples_per_second": 63.161
    },
    "gaudi3": {
      "perplexity": 1.0262332298756216,
      "train_runtime": 20.7457,
      "train_samples_per_second": 60.479
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingPrefixTuningExampleTester::test_run_prompt_tuning_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 1.172,
      "train_runtime": 16.1,
      "train_samples_per_second": 63.249
    },
    "gaudi3": {
      "perplexity": 1.1720024747280242,
      "train_runtime": 17.2662,
      "train_samples_per_second": 61.545
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingPromptTuningExampleTester::test_run_prompt_tuning_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 1.224,
      "train_runtime": 16.5,
      "train_samples_per_second": 63.161
    },
    "gaudi3": {
      "perplexity": 1.2158095633720596,
      "train_runtime": 16.7016,
      "train_samples_per_second": 61.478
    }
  },
  "tests/test_examples.py::MultiCardCausalLanguageModelingVeraExampleTester::test_run_lora_clm_llama-7b_multi_card": {
    "gaudi2": {
      "perplexity": 2.6174,
      "train_runtime": 312.3,
      "train_samples_per_second": 125.798
    },
    "gaudi3": {
      "perplexity": 2.613,
      "train_runtime": 321.3809,
      "train_samples_per_second": 168.296
    }
  },
  "tests/test_examples.py::MultiCardDPOExampleTester::test_dpo_llama-7b_multi_card": {
    "gaudi2": {
      "train_runtime": 234.6471,
      "train_samples_per_second": 13.499
    },
    "gaudi3": {
      "train_runtime": 211.3438,
      "train_samples_per_second": 15.141
    }
  },
  "tests/test_examples.py::MultiCardImageClassificationExampleTester::test_run_image_classification_swin-base-patch4-window7-224-in22k_multi_card": {
    "gaudi1": {
      "eval_accuracy": 0.9819,
      "train_runtime": 117.6424,
      "train_samples_per_second": 1683.344
    },
    "gaudi2": {
      "eval_accuracy": 0.9821,
      "train_runtime": 62.9986,
      "train_samples_per_second": 6202.525
    },
    "gaudi3": {
      "eval_accuracy": 0.9817333333333333,
      "train_runtime": 106.7478,
      "train_samples_per_second": 5241.263
    }
  },
  "tests/test_examples.py::MultiCardImageClassificationExampleTester::test_run_image_classification_vit-base-patch16-224-in21k_multi_card": {
    "gaudi1": {
      "eval_accuracy": 0.9803,
      "train_runtime": 59.972,
      "train_samples_per_second": 2508.955
    },
    "gaudi2": {
      "eval_accuracy": 0.9679,
      "train_runtime": 23.99,
      "train_samples_per_second": 6718.643
    },
    "gaudi3": {
      "eval_accuracy": 0.9677333333333333,
      "train_runtime": 38.7232,
      "train_samples_per_second": 6636.054
    }
  },
  "tests/test_examples.py::MultiCardImageToTextModelingLoRAExampleTester::test_run_image2text_lora_finetune_Llama-3.2-11B-Vision-Instruct_multi_card": {
    "gaudi2": {
      "eval_accuracy": 0.6,
      "train_runtime": 350,
      "train_samples_per_second": 20.48
    },
    "gaudi3": {
      "eval_accuracy": 0.9044574025188373,
      "train_runtime": 583.0969,
      "train_samples_per_second": 35.056
    }
  },
  "tests/test_examples.py::MultiCardImageToTextModelingLoRAExampleTester::test_run_image2text_lora_finetune_idefics2-8b_multi_card": {
    "gaudi2": {
      "eval_accuracy": 0.6,
      "train_runtime": 286,
      "train_samples_per_second": 11.8
    },
    "gaudi3": {
      "eval_accuracy": 0.6,
      "train_runtime": 273.7778,
      "train_samples_per_second": 17.93
    }
  },
  "tests/test_examples.py::MultiCardImageToTextModelingLoRAExampleTester::test_run_image2text_lora_finetune_llava-1.5-7b-hf_multi_card": {
    "gaudi2": {
      "eval_accuracy": 0.21,
      "train_runtime": 118.5782,
      "train_samples_per_second": 25.146
    },
    "gaudi3": {
      "eval_accuracy": 0.20785648331296863,
      "train_runtime": 184.9003,
      "train_samples_per_second": 27.828
    }
  },
  "tests/test_examples.py::MultiCardMaskedLanguageModelingExampleTester::test_run_mlm_roberta-large_multi_card": {
    "gaudi1": {
      "perplexity": 2.7851,
      "train_runtime": 75.0033,
      "train_samples_per_second": 217.752
    },
    "gaudi2": {
      "perplexity": 2.829522488584474,
      "train_runtime": 22.7101,
      "train_samples_per_second": 1056.875
    },
    "gaudi3": {
      "perplexity": 2.8534683742096933,
      "train_runtime": 109.6454,
      "train_samples_per_second": 994.854
    }
  },
  "tests/test_examples.py::MultiCardPPOExampleTester::test_ppo_llama-7b_multi_card": {
    "gaudi2": {
      "train_runtime": 62,
      "train_samples_per_second": 0.5
    },
    "gaudi3": {
      "train_runtime": 47.45752716064453,
      "train_samples_per_second": 0.6742871345082827
    }
  },
  "tests/test_examples.py::MultiCardProteinFoldingClassificationTester::test_run_sequence_classification_protst-esm1b-for-sequential-classification_multi_card": {
    "gaudi2": {
      "eval_accuracy": 0.5436668594563332,
      "train_runtime": 38.9504,
      "train_samples_per_second": 768.648
    },
    "gaudi3": {
      "eval_accuracy": 0.5442452284557547,
      "train_runtime": 50.2717,
      "train_samples_per_second": 1564.079
    }
  },
  "tests/test_examples.py::MultiCardQuestionAnsweringExampleTester::test_run_qa_roberta-large_multi_card": {
    "gaudi1": {
      "eval_f1": 94.2867,
      "train_runtime": 304.9084,
      "train_samples_per_second": 366.177
    },
    "gaudi2": {
      "eval_f1": 94.09,
      "train_runtime": 79.333,
      "train_samples_per_second": 2138.366
    },
    "gaudi3": {
      "eval_f1": 94.33668918864852,
      "train_runtime": 184.2499,
      "train_samples_per_second": 3146.332
    }
  },
  "tests/test_examples.py::MultiCardRewardExampleTester::test_reward_modeling_llama-7b_multi_card": {
    "gaudi2": {
      "train_runtime": 250,
      "train_samples_per_second": 1.6
    },
    "gaudi3": {
      "train_runtime": 169.1724,
      "train_samples_per_second": 2.418
    }
  },
  "tests/test_examples.py::MultiCardSFTChatExampleTester::test_sft_Qwen2-7B_multi_card": {
    "gaudi2": {
      "train_runtime": 423.995,
      "train_samples_per_second": 7.342
    },
    "gaudi3": {
      "train_runtime": 693.6076,
      "train_samples_per_second": 13.968
    }
  },
  "tests/test_examples.py::MultiCardSFTChatPeftExampleTester::test_sft_Qwen2-7B_multi_card": {
    "gaudi2": {
      "train_runtime": 410,
      "train_samples_per_second": 120
    },
    "gaudi3": {
      "train_runtime": 383.0226,
      "train_samples_per_second": 193.023
    }
  },
  "tests/test_examples.py::MultiCardSFTExampleTester::test_sft_llama-7b_multi_card": {
    "gaudi2": {
      "train_runtime": 206,
      "train_samples_per_second": 51.54
    },
    "gaudi3": {
      "train_runtime": 347.6082,
      "train_samples_per_second": 86.193
    }
  },
  "tests/test_examples.py::MultiCardSeq2SeqSpeechRecognitionExampleTester::test_run_speech_recognition_seq2seq_whisper-small_multi_card": {
    "gaudi1": {
      "eval_samples_per_second": 6.851,
      "eval_wer": 2.1133,
      "train_runtime": 551.3249,
      "train_samples_per_second": 145.59
    },
    "gaudi2": {
      "eval_samples_per_second": 31.0,
      "eval_wer": 0.4693843594009983,
      "train_runtime": 380.0,
      "train_samples_per_second": 218.0
    },
    "gaudi3": {
      "eval_samples_per_second": 48.493,
      "eval_wer": 0.4296589018302829,
      "train_runtime": 351.7181,
      "train_samples_per_second": 417.561
    }
  },
  "tests/test_examples.py::MultiCardSpeechRecognitionExampleTester::test_run_speech_recognition_ctc_wav2vec2-large-lv60_multi_card": {
    "gaudi1": {
      "eval_samples_per_second": 54.189,
      "eval_wer": 0.0496,
      "train_runtime": 984.3022,
      "train_samples_per_second": 63.043
    },
    "gaudi2": {
      "eval_samples_per_second": 196.665,
      "eval_wer": 0.1109,
      "train_runtime": 308.8036,
      "train_samples_per_second": 225.572
    },
    "gaudi3": {
      "eval_samples_per_second": 369.034,
      "eval_wer": 0.06197937326457755,
      "train_runtime": 235.8412,
      "train_samples_per_second": 392.016
    }
  },
  "tests/test_examples.py::MultiCardTextClassificationExampleTester::test_run_glue_bert-large-uncased-whole-word-masking_multi_card": {
    "gaudi1": {
      "eval_f1": 0.8897,
      "train_runtime": 65.644,
      "train_samples_per_second": 919.623
    },
    "gaudi2": {
      "eval_f1": 0.8452579034941764,
      "train_runtime": 31.445,
      "train_samples_per_second": 2845.068
    },
    "gaudi3": {
      "eval_f1": 0.8809523809523809,
      "train_runtime": 88.3481,
      "train_samples_per_second": 1826.566
    }
  },
  "tests/test_examples.py::MultiCardVisionLanguageExampleTester::test_run_clip_clip-roberta_multi_card": {
    "gaudi1": {
      "train_runtime": 314.7726,
      "train_samples_per_second": 2560.999
    },
    "gaudi2": {
      "train_runtime": 59.5,
      "train_samples_per_second": 14124
    },
    "gaudi3": {
      "train_runtime": 71.4956,
      "train_samples_per_second": 20525.583
    }
  },
  "tests/test_examples.py::QuestionAnsweringExampleTester::test_run_qa_roberta-large_single_card": {
    "gaudi1": {
      "eval_f1": 94.2959,
      "train_runtime": 1771.3319,
      "train_samples_per_second": 50.815
    },
    "gaudi2": {
      "eval_f1": 94.5886,
      "train_runtime": 361.4789,
      "train_samples_per_second": 266.47
    },
    "gaudi3": {
      "eval_f1": 94.36192902198283,
      "train_runtime": 276.0212,
      "train_samples_per_second": 400.151
    }
  },
  "tests/test_examples.py::TextClassificationExampleTester::test_run_glue_bert-large-uncased-whole-word-masking_single_card": {
    "gaudi1": {
      "eval_f1": 0.9022,
      "train_runtime": 90.3943,
      "train_samples_per_second": 172.792
    },
    "gaudi2": {
      "eval_f1": 0.867,
      "train_runtime": 33.2909,
      "train_samples_per_second": 1100.598
    },
    "gaudi3": {
      "eval_f1": 0.8826,
      "train_runtime": 85.085,
      "train_samples_per_second": 1674.509
    }
  }
}