{
    "gaudi": {
        "squad": {
            "num_train_epochs": 1,
            "eval_batch_size": 8,
            "distribution": {
                "single_card": {
                    "learning_rate": 1e-4,
                    "train_batch_size": 48,
                    "eval_f1": 84.7137,
                    "train_runtime": 271.2751,
                    "train_samples_per_second": 334.792,
                    "extra_arguments": [
                        "--max_seq_length 384",
                        "--use_hpu_graphs_for_inference"
                    ]
                },
                "multi_card": {
                    "learning_rate": 3e-4,
                    "train_batch_size": 48,
                    "eval_f1": 82.8831,
                    "train_runtime": 54.0269,
                    "train_samples_per_second": 2500.721,
                    "extra_arguments": [
                        "--max_seq_length 384",
                        "--use_hpu_graphs_for_inference"
                    ]
                }
            }
        }
    },
    "gaudi2": {
        "squad": {
            "num_train_epochs": 1,
            "eval_batch_size": 8,
            "distribution": {
                "single_card": {
                    "learning_rate": 2e-4,
                    "train_batch_size": 64,
                    "eval_f1": 84.2868,
                    "train_runtime": 70.1056,
                    "train_samples_per_second": 1321.639,
                    "extra_arguments": [
                        "--max_seq_length 384",
                        "--use_hpu_graphs_for_inference"
                    ]
                },
                "multi_card": {
                    "learning_rate": 3e-4,
                    "train_batch_size": 64,
                    "eval_f1": 82.442,
                    "train_runtime": 16.9833,
                    "train_samples_per_second": 9917.008,
                    "extra_arguments": [
                        "--max_seq_length 384",
                        "--use_hpu_graphs_for_inference"
                    ]
                }
            }
        }
    }
}