{
    "gaudi2": {
        "trl-sft-chat-peft": {
            "num_train_epochs": 1,
            "eval_batch_size": 32,
            "distribution": {
                "multi_card": {
                    "learning_rate": 3e-4,
                    "train_batch_size": 32,
                    "metrics": ["train_runtime", "train_samples_per_second"],
                    "extra_arguments": [
                        "--bf16 True",
                        "--subset ''",
                        "--streaming False",
                        "--packing True",
                        "--gradient_accumulation_steps 8",
                        "--gradient_checkpointing True",
                        "--eval_strategy no",
                        "--save_strategy no",
                        "--throughput_warmup_steps 5",
                        "--warmup_ratio  0.03",
                        "--lr_scheduler_type cosine",
                        "--max_grad_norm  0.3",
                        "--logging_steps 1",
                        "--adam_epsilon 3e-4",
                        "--use_peft True",
                        "--lora_r 4",
                        "--lora_alpha 16",
                        "--lora_dropout 0.05",
                        "--lora_target_modules q_proj v_proj k_proj o_proj",
                        "--max_seq_length 512",
                        "--weight_decay 0.05",
                        "--report_to none",
                        "--max_steps 20"
                    ]
                }
            }
        },
        "trl-sft-chat": {
            "num_train_epochs": 1,
            "eval_batch_size": 2,
            "distribution": {
                "multi_card": {
                    "learning_rate": 3e-4,
                    "train_batch_size": 2,
                    "metrics": ["train_runtime", "train_samples_per_second"],
                    "extra_arguments": [
                        "--bf16 True",
                        "--subset ''",
                        "--streaming False",
                        "--packing True",
                        "--gradient_accumulation_steps 8",
                        "--gradient_checkpointing True",
                        "--eval_strategy no",
                        "--save_strategy no",
                        "--throughput_warmup_steps 5",
                        "--warmup_ratio  0.03",
                        "--lr_scheduler_type cosine",
                        "--max_grad_norm  0.3",
                        "--logging_steps 1",
                        "--adam_epsilon 3e-4",
                        "--use_peft False",
                        "--max_seq_length 4096",
                        "--report_to none",
                        "--use_flash_attention True",
                        "--max_steps 20"
                    ]
                }
            }
        }
    }
}
