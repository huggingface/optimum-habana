# {{ template "chart.name" . }}

{{ template "chart.badgesSection" . }}

This folder contains a Dockerfile and [Helm chart](https://helm.sh) demonstrating how ðŸ¤— Optimum Habana examples
can be run using IntelÂ® GaudiÂ® AI accelerator nodes from a vanilla Kubernetes cluster. The instructions below
explain how to build the docker image and deploy the job to a Kubernetes cluster using Helm.

## Requirements

### Client System Requirements

In order to build the docker images and deploy a job to Kubernetes you will need:

* [Docker](https://docs.docker.com/engine/install/)
* [Docker compose](https://docs.docker.com/compose/install/)
* [kubectl](https://kubernetes.io/docs/tasks/tools/) installed and configured to access a cluster
* [Optional] [Minikube](https://minikube.sigs.k8s.io/docs/start/) installed and configured to access a Kubernetes cluster on a single node
* [Helm CLI](https://helm.sh/docs/intro/install/)

> If you are intending to run a Kubernetes cluster on a single node (locally), you will need a tool like [Minikube](https://minikube.sigs.k8s.io/docs/start/) to be installed first

### Cluster Requirements

Your Kubernetes cluster will need the [Intel Gaudi Device Plugin for Kubernetes](https://docs.habana.ai/en/latest/Orchestration/Gaudi_Kubernetes/Device_Plugin_for_Kubernetes.html)
in order to request and utilize the accelerators in the Kubernetes job. Also, ensure that your Kubernetes version is supported based on the
[support matrix](https://docs.habana.ai/en/latest/Support_Matrix/Support_Matrix.html#support-matrix).

## Container

The [Dockerfile](Dockerfile) and [docker-compose.yaml](docker-compose.yaml) build the following images:

* An `optimum-habana` base image that uses the [PyTorch Docker images for Gaudi](https://developer.habana.ai/catalog/pytorch-container/) as it's base, and then installs
optimum-habana and the Habana fork of Deep Speed.
* An `optimum-habana-examples` image is built on top of the `optimum-habana` base to includes installations from
`requirements.txt` files in the example directories and a clone of [this GitHub repository](https://github.com/huggingface/optimum-habana/) in order to run example scripts.

Use the the following commands to build the containers:

> Note that the `GAUDI_SW_VER`, `OS`, and `TORCH_VER` are used to
> determine which [Intel Gaudi PyTorch base image](https://developer.habana.ai/catalog/pytorch-container/) to use. The
> combination of versions provided must match one of the pre-built images that are available in the Intel Gaudi Vault.

```bash
# Specify the Gaudi SW version, OS, and PyTorch version which will be used for the base container
export GAUDI_SW_VER=1.16.2
export OS=ubuntu22.04
export TORCH_VER=2.2.2

# Specify the version of optimum-habana to install in the container
export OPTIMUM_HABANA_VER=1.12.1

git clone https://github.com/huggingface/optimum-habana.git

# Note: Modify the requirements.txt file in the kubernetes directory for the specific example(s) that you want to run
cd optimum-habana/examples/kubernetes

# Set variables for your container registry and repository
export REGISTRY=<Your container registry>
export REPO=<Your container repository>

# Build the images
docker compose build

# Push the optimum-habana-examples image to a container registry
docker push <image name>:<tag>
```

## Helm Chart

### Kubernetes resources

This Kubernetes job uses a [Helm chart](https://helm.sh) with the following resources:
* Job to run the Optimum Habana example script using HPU(s) from a single node
* [Persistant volume claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims)
  (PVC) backed by NFS to store output files
* (Optional) Pod used to access the files from the PVC after the worker pod completes
* (Optional) [Secret](https://kubernetes.io/docs/concepts/configuration/secret/) for a Hugging Face token, if gated
  models are being used

### Helm chart values

The [Helm chart values file](https://helm.sh/docs/chart_template_guide/values_files/) is a yaml file with values that
get passed to the chart when it's deployed to the cluster. These values specify the python script and parameters for
your job, the name and tag of your Docker image, the number of HPU cards to use for the job, etc.

<details>
  <summary> Expand to see the values table </summary>

{{ template "chart.valuesTable" . }}

</details>

There is a [`values.yaml` files](values.yaml) that is intended to be used as a template to run an optimum-habana
example script. Ensure that your container has the requirements needed to run the example, and then update the
`values.yaml` file with your `image.repository` and `image.tag`. Then, update the `command` array with the script and
parameters to run the example.

Validated use cases can be found in the `ci` directory:

| Values files path | HPUs | Description |
|-------------------|------|-------------|
| [`ci/single-card-glue-values.yaml`](ci/single-card-glue-values.yaml) | 1 | Uses a single card to [fine tune BERT large](../text-classification/README.md#single-card-training) (with whole word masking) on the text classification MRPC task using `run_glue.py`.
| [`ci/multi-card-glue-values.yaml`](ci/multi-card-glue-values.yaml) | 2 | Uses 2 HPUs from a single node with the [`gaudi_spawn.py`](../gaudi_spawn.py) script to [fine tune BERT large](../text-classification/README.md#multi-card-training) (with whole word masking) on the text classification MRPC task using `run_glue.py`.
| [`ci/single-card-lora-clm-values.yaml`](ci/single-card-lora-clm-values.yaml) | 1 | Uses a single card to [fine tune Llama1-7B](../language-modeling/README.md#peft) with LoRA using the `run_lora_clm.py` script.
| [`ci/multi-card-lora-clm-values.yaml`](ci/multi-card-lora-clm-values.yaml) | 8 | Uses 8 HPUs from a single node with the [`gaudi_spawn.py`](../gaudi_spawn.py) script to [fine tune Llama1-7B](../language-modeling/README.md#peft) with LoRA using the `run_lora_clm.py` script.

### Deploy job to the cluster

After updating the values file for the example that you want to run, use the following command to deploy the job to
your Kubernetes cluster.

```bash
cd examples/kubernetes

helm install -f <path to values.yaml> optimum-habana-examples . -n <namespace>
```

After the job is deployed, you can check the status of the pods:
```bash
kubectl get pods -n <namespace>
```

To monitor a running job, you can view the logs of the worker pod:

```bash
kubectl logs <pod name> -n <namespace> -f
```

The data access pod can be used to copy artifacts from the persistent volume claim (for example, the trained model
after fine tuning completes). Note that this requires that the data access pod be deployed to the cluster with the helm
chart by setting `storage.deployDataAccessPod = true` in the values yaml file. The path to the files is defined in the
`storage.pvcMountPath` value (this defaults to `/tmp/pvc-mount`). You can find the name of your data access pod using
`kubectl get pods -n <namespace> | grep dataaccess`.

```bash
# Copy files from the PVC mount path to your local machine
kubectl cp <data access pod name>:/tmp/pvc-mount . -n <namespace>
```

Finally, when your job is complete and you've copied all the artifacts that you need to your local machine, you can
uninstall the helm job from the cluster:

```bash
helm uninstall optimum-habana-examples . -n <namespace>
```

{{ template "helm-docs.versionFooter" . }}
