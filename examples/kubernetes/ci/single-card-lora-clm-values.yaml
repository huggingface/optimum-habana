# Default values for examples.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image:
  # -- Determines when the kubelet will pull the image to the worker nodes. Choose from: `IfNotPresent`, `Always`, or `Never`. If updates to the image have been made, use `Always` to ensure the newest image is used.
  pullPolicy: IfNotPresent
  # -- Repository and name of the docker image
  repository: 
  # -- Tag of the docker image
  tag: 

imagePullSecrets: []

# -- Pod [annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) to attach metadata to the job
podAnnotations: {}

# -- Specify a pod security context to run as a non-root user
podSecurityContext: {}
  # runAsUser: 1000
  # runAsGroup: 3000
  # fsGroup: 2000

securityContext:
  # -- Run as privileged or unprivileged. Certain deployments may require running as privileged, check with your system admin.
  privileged: false

# -- The default 64MB of shared memory for docker containers can be insufficient when using more than one HPU. Setting hostIPC: true allows reusing the host's shared memory space inside the container.
hostIPC: false

# -- Define a config map's data as container environment variables
envFrom: []

# -- Define environment variables to set in the container
env:
- name: LOGLEVEL
  value: INFO

secret:
  # -- Hugging Face token encoded using base64.
  encodedToken:
  # -- If a token is provided, specify a mount path that will be used to set HF_TOKEN_PATH
  secretMountPath: /tmp/hf_token

storage:
  # -- Name of the storage class to use for the persistent volume claim. To list the available storage classes use: `kubectl get storageclass`.
  storageClassName: nfs-client
  # -- [Access modes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) for the persistent volume.
  accessModes:
  - "ReadWriteMany"
  # -- Storage [resources](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#resources)
  resources:
    requests:
      storage: 30Gi
  # -- Locaton where the PVC will be mounted in the pods
  pvcMountPath: &pvcMountPath /tmp/pvc-mount
  # -- A data access pod will be deployed when set to true
  deployDataAccessPod: true

resources:
  limits:
    # -- Specify the number of Gaudi card(s)
    habana.ai/gaudi: &hpus 1
    # -- Specify [CPU resource](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu) limits for the job
    cpu: 16
    # -- Specify [Memory limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory) requests for the job
    memory: 128Gi
    # -- Specify hugepages-2Mi requests for the job
    hugepages-2Mi: 4400Mi
  requests:
    # -- Specify the number of Gaudi card(s)
    habana.ai/gaudi: *hpus
    # -- Specify [CPU resource](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-cpu) requests for the job
    cpu: 16
    # -- Specify [Memory resource](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory) requests for the job
    memory: 128Gi
    # -- Specify hugepages-2Mi requests for the job
    hugepages-2Mi: 4400Mi

# Define the command to run in the container
command:
  - python
  - /workspace/optimum-habana/examples/language-modeling/run_lora_clm.py 
  - --model_name_or_path
  - huggyllama/llama-7b 
  - --dataset_name
  - tatsu-lab/alpaca 
  - --bf16=True
  - --output_dir
  - *pvcMountPath
  - --num_train_epochs
  - "3"
  - --per_device_train_batch_size
  - "16"
  - --evaluation_strategy
  - "no" 
  - --save_strategy
  - "no" 
  - --learning_rate
  - "1e-4" 
  - --warmup_ratio
  - "0.03" 
  - --lr_scheduler_type
  - "constant" 
  - --max_grad_norm
  - "0.3" 
  - --logging_steps
  - "1"
  - --do_train 
  - --do_eval 
  - --use_habana 
  - --use_lazy_mode 
  - --throughput_warmup_steps
  - "3"
  - --lora_rank
  - "8" 
  - --lora_alph=16 
  - --lora_dropout=0.05 
  - --lora_target_modules
  - "q_proj"
  - "v_proj" 
  - --dataset_concatenation 
  - --max_seq_length=512 
  - --low_cpu_mem_usage=True 
  - --validation_split_percentage=4 
  - --adam_epsilon=1e-08

# -- Optionally specify a [node selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) with labels the determine which node your worker pod will land on
nodeSelector: {}

# -- Optionally specify [tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to allow the worker pod to land on a node with a taint.
tolerations: []

# -- Optionally provide node [affinities](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) to constrain which node your worker pod will be scheduled on
affinity: {}
