<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->


# Quickstart

ðŸ¤— Optimum Habana was designed with one goal in mind: **making training and evaluation straightforward for any ðŸ¤— Transformers user while leveraging the complete power of Gaudi processors**.
There are two main classes one needs to know:
- [`GaudiTrainer`](https://huggingface.co/docs/optimum/habana/package_reference/trainer): the trainer class that takes care of compiling (lazy or eager mode) and distributing the model to run on HPUs, and of performing training and evaluation.
- [`GaudiConfig`](https://huggingface.co/docs/optimum/habana/package_reference/gaudi_config): the class that enables to configure Habana Mixed Precision and to decide whether optimized operators and optimizers should be used or not.

The [`GaudiTrainer`](https://huggingface.co/docs/optimum/habana/package_reference/trainer) is very similar to the [ðŸ¤— Transformers Trainer](https://huggingface.co/docs/transformers/main_classes/trainer), and adapting a script using the Trainer to make it work with Gaudi will mostly consist in simply swapping the `Trainer` class for the `GaudiTrainer` one.
That is how most of the [example scripts](https://github.com/huggingface/optimum-habana/tree/main/examples) were adapted from their [original counterparts](https://github.com/huggingface/transformers/tree/main/examples/pytorch).

```diff
- from transformers import Trainer, TrainingArguments
+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments

# Define the training arguments
- training_args = TrainingArguments(
+ training_args = GaudiTrainingArguments(
+   use_habana=True,
+   use_lazy_mode=True,
+   gaudi_config_name=gaudi_config_name,
  ...
)

# Initialize our Trainer
- trainer = Trainer(
+ trainer = GaudiTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset
    ... # other arguments
)
```

where `gaudi_config_name` is the name of a model from the [Hub](https://huggingface.co/Habana) (Gaudi configurations are stored in model repositories) or a path to a local Gaudi configuration file (you can see [here](./package_reference/gaudi_config) how to write your own).


## Stable Diffusion

ðŸ¤— Optimum Habana also features HPU-optimized support for the ðŸ¤— Diffusers library.
Thus, you can easily deploy Stable Diffusion on Gaudi for performing text-to-image generation.

Here is how to use it and the differences with the ðŸ¤— Diffusers library:
```diff
- from diffusers import DDIMScheduler, StableDiffusionPipeline
+ from optimum.habana.diffusers import GaudiDDIMScheduler, GaudiStableDiffusionPipeline


model_name = "CompVis/stable-diffusion-v1-4"

- scheduler = DDIMScheduler.from_pretrained(model_name, subfolder="scheduler")
+ scheduler = GaudiDDIMScheduler.from_pretrained(model_name, subfolder="scheduler")

- pipeline = StableDiffusionPipeline.from_pretrained(
+ pipeline = GaudiStableDiffusionPipeline.from_pretrained(
    model_name,
    scheduler=scheduler,
+   use_habana=True,
+   use_hpu_graphs=True,
+   gaudi_config="Habana/stable-diffusion",
)

outputs = pipeline(
    ["An image of a squirrel in Picasso style"],
    num_images_per_prompt=16,
+   batch_size=4,
)
```


## Ready-to-Use Examples

Here are examples for various modalities and tasks that can be used out of the box:
- Text
  - [text classification](https://github.com/huggingface/optimum-habana/tree/main/examples/text-classification),
  - [question answering](https://github.com/huggingface/optimum-habana/tree/main/examples/question-answering),
  - [language modeling](https://github.com/huggingface/optimum-habana/tree/main/examples/language-modeling),
  - [text generation](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation),
  - [summarization](https://github.com/huggingface/optimum-habana/tree/main/examples/summarization),
  - [translation](https://github.com/huggingface/optimum-habana/tree/main/examples/translation),
  - [protein folding](https://github.com/huggingface/optimum-habana/tree/main/examples/protein-folding)
- Images
  - [image classification](https://github.com/huggingface/optimum-habana/tree/main/examples/image-classification)
- Audio
  - [audio classification](https://github.com/huggingface/optimum-habana/tree/main/examples/audio-classification),
  - [speech recognition](https://github.com/huggingface/optimum-habana/tree/main/examples/speech-recognition)
- Text and images
  - [text-to-image generation](https://github.com/huggingface/optimum-habana/tree/main/examples/stable-diffusion),
  - [contrastive image-text training](https://github.com/huggingface/optimum-habana/tree/main/examples/contrastive-image-text).
