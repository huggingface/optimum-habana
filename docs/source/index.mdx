<!---
Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->


# ğŸ¤— Optimum Habana

ğŸ¤— Optimum Habana is the interface between the ğŸ¤— Transformers library and [Habana's Gaudi processor (HPU)](https://docs.habana.ai/en/latest/index.html). It provides a set of tools enabling easy model loading and training on single- and multi-HPU settings for different downstream tasks.
The list of officially validated models and tasks is available [here](https://huggingface.co/docs/optimum/main/en/habana_index#validated-models). Users can try other models and tasks with only few changes.

## What is a Habana Processing Unit (HPU)?

Quote from the Hugging Face [blog post](https://huggingface.co/blog/habana):

> Habana Gaudi training solutions, which power Amazon's EC2 DL1 instances and Supermicro's X12 Gaudi AI Training Server, deliver price/performance up to 40% lower than comparable training solutions and enable customers to train more while spending less. The integration of ten 100 Gigabit Ethernet ports onto every Gaudi processor enables system scaling from 1 to thousands of Gaudis with ease and cost-efficiency. Habanaâ€™s SynapseAIÂ® is optimizedâ€”at inceptionâ€”to enable Gaudi performance and usability, supports TensorFlow and PyTorch frameworks, with a focus on computer vision and natural language processing applications.


## Install
To install the latest release of this package:

```bash
pip install optimum[habana]
```

<Tip>

To use DeepSpeed on HPUs, you also need to run the following command:
```bash
pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.6.1
```

</Tip>

Optimum Habana is a fast-moving project, and you may want to install it from source:

```bash
pip install git+https://github.com/huggingface/optimum-habana.git
```

<Tip>

Alternatively, you can install the package without pip as follows:
```bash
git clone https://github.com/huggingface/optimum-habana.git
cd optimum-habana
python setup.py install
```

</Tip>

Last but not least, don't forget to install requirements for every example:

```bash
cd <example-folder>
pip install -r requirements.txt
```


## Validated Models

The following model architectures, tasks and device distributions have been validated for ğŸ¤— Optimum Habana:

|            | Text Classification | Question Answering | Language Modeling  | Summarization | Translation | Image Classification | Single Card | Multi Card | DeepSpeed |
|------------|:-------------------:|:------------------:|:------------------:|:-------------:|:-----------:|:--------------------:|:-----------:|:----------:|:---------:|
| BERT       | âœ…                  | âœ…                 | âœ…                 | âŒ            | âŒ          | âŒ                   | âœ…          | âœ…         | âœ…        |
| RoBERTa    | âŒ                  | âœ…                 | âœ…                 | âŒ            | âŒ          | âŒ                   | âœ…          | âœ…         | âœ…        |
| ALBERT     | âŒ                  | âœ…                 | âœ…                 | âŒ            | âŒ          | âŒ                   | âœ…          | âœ…         | âœ…        |
| DistilBERT | âŒ                  | âœ…                 | âœ…                 | âŒ            | âŒ          | âŒ                   | âœ…          | âœ…         | âœ…        |
| GPT2       | âŒ                  | âŒ                 | âœ…                 | âŒ            | âŒ          | âŒ                   | âœ…          | âœ…         | âœ…        |
| T5         | âŒ                  | âŒ                 | âŒ                 | âœ…            | âœ…          | âŒ                   | âœ…          | âœ…         | âœ…        |
| ViT        | âŒ                  | âŒ                 | âŒ                 | âŒ            | âŒ          | âœ…                   | âœ…          | âœ…         | âœ…        |
| Swin       | âŒ                  | âŒ                 | âŒ                 | âŒ            | âŒ          | âœ…                   | âœ…          | âœ…         | âœ…        |

Other models and tasks supported by the ğŸ¤— Transformers library may also work. You can refer to this [section](quickstart) for using them with ğŸ¤— Optimum Habana. Besides, [this page](https://github.com/huggingface/optimum-habana/tree/main/examples) explains how to modify any [example](https://github.com/huggingface/transformers/tree/main/examples/pytorch) from the ğŸ¤— Transformers library to make it work with ğŸ¤— Optimum Habana.

If you find any issue while using those, please open an issue or a pull request in the [Github repository of the project](https://github.com/huggingface/optimum-habana).


## Gaudi Setup

Please refer to Habana Gaudi's official [installation guide](https://docs.habana.ai/en/latest/Installation_Guide/index.html).

<Tip>

Tests should be run in a Docker container based on Habana Docker images.

The current version has been validated for SynapseAI 1.6.

</Tip>
